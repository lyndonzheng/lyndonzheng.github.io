<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chuanxia Zheng</title>

  <meta name="author" content="Chuanxia Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chuanxia Zheng</name>
              </p>
              <p> <a href="https://eng.ox.ac.uk/people/chuanxia-zheng/"> Chuanxia Zheng </a> is a postdoctoral researcher in <a href="https://www.robots.ox.ac.uk/~vgg/"> VGG </a> at the <a href="https://www.ox.ac.uk/">University of Oxford</a>, working with  Prof. <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> on 3D reconstruction and generation and Prof. <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a> on generative model for understanding.
              </p>
              <p>
                Before that, he spent one year at <a href="https://www.monash.edu/">Monash University</a>, where he worked as a Research Fellow with Prof. <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> and Prof. <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a> on codebook learning for generation. He received his PhD degree from the <a href="https://www.ntu.edu.sg/scse"> SCSE </a> at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>, supervised by Prof. <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a> and Prof. <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> on 2D generation, translation and completion. His thesis <a href="https://arxiv.org/abs/2202.12752">Synthesizing Photorealistic Images</a> was awarded the <a href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-outstanding-phd-thesis-award-2022"> NTU Outstanding PhD Thesis Award 2022</a>. <!--Before that, I received the Master’s degree in the <a href="http://irmct.buaa.edu.cn/"> IR&MCT Lab </a> at Beihang University, advised by Jianhua Wang and <a href="http://dept3.buaa.edu.cn/szjs/zzjs/dgdzjxsyzx1/js/cwh.htm"> Weihai Chen</a> -->
              </p>
              <p style="text-align:center">
                <a href="CHUANXIA001@e.ntu.edu.sg">Email</a> &nbsp/&nbsp
                <a href="data/cv_chuanxia.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mvpE6bIAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/lyndonzheng">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/ChuanxiaZ">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <div class="news">
              <ul style="text-align:justify;height: 150px;">
              <li class="font"><font color="red">2023.07: <a target="_blank" href="https://chuanxiaz.com/cvq">CVQ-VAE</a> was accepted by ICCV2023.</font></li>
              <li class="font"><font color="red">2023.05: I was named one of the CVPR2023 Outstanding Reviewers.</a> </font></li>
              <li class="font">2023.04: <a target="_blank" href="https://chuanxiaz.com/movq">MoVQ</a> was reported means a lot to <a target="_blank" href="https://habr.com/ru/companies/sberbank/articles/725282/">Kandinsky2.1.</a>, <a target="_blank" href="https://github.com/ai-forever/Kandinsky-2">Github.</a></font></li>
              <li class="font">2023.04: <a target="_blank" href="https://chuanxiaz.com">VQ-WAE</a> was accepted by ICML2023.</font></li>
              <li class="font">2023.01: <a target="_blank" href="https://mhh0318.github.io/unid3/">UniD3</a> was accepted by ICLR2023.</font></li>
              <li class="font">2022.12: I started my research in <a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/">VGG</a> at University of Oxford.</font></li>
              <li class="font">2022.09: <a target="_blank" href="https://chuanxiaz.com/movq">MoVQ</a> was accepted by NeurIPS2022.</font></li>
              <li class="font">2022.08: <a target="_blank" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0271056">Nuclear Predication</a> was accepted by PLOS ONE.</font></li>
              <li class="font">2022.07: <a target="_blank" href="https://wuqianyi.top/objectsdf/">ObjectSDF</a> and <a target="_blank" href="https://donydchen.github.io/sem2nerf/">Semantic2NeRF</a> were accepted by ECCV2022.</font></li>
              <li class="font">2022.06: I was the winner of the <a target="_blank" href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-outstanding-phd-thesis-award-2022">NTU Outstanding PhD Thesis Award 2022</a>.</font></li>
              <li class="font">2022.03: <a target="_blank" href="https://mp.weixin.qq.com/s/e1gqtDsH872sjfH4T4oL2w?scene=25#wechat_redirect">Synthesizing Photorealistic Scenes</a> was reported on media.</font></li>
              <li class="font">2022.03: <a target="_blank" href="https://chuanxiaz.com/tfill">TFill</a> was accepted by CVPR2022.</font></li>
              <li class="font">2021.08: <a target="_blank" href="https://chuanxiaz.com/vinv">Visiting the Invisible</a> was accepted by IJCV.</font></li>
              <li class="font">2021.07: One paper was accepted by ICCV2021.</font></li>
              <li class="font">2021.06: <a target="_blank" href="https://link.springer.com/article/10.1007/s11263-021-01502-7">Pluralistic Free-Form Image Completion </a> was accepted by IJCV.</font></li>
              <li class="font">2021.06: <a target="_blank" href="https://guoxiansong.github.io/homepage/agilegan.html">AgileGAN</a> was reported by <a target="_blank" href="https://www.sohu.com/a/471951100_629135">Sohu</a> and reproduced by <a target="_blank" href="https://github.com/open-mmlab/MMGEN-FaceStylor">Open MMLab</a>.</font></li>
              <li class="font">2021.04: <a target="_blank" href="https://guoxiansong.github.io/homepage/agilegan.html">AgileGAN</a> was accepted by SIGGRAPH2021.</font></li>
              <li class="font">2021.03: <a target="_blank" href="https://chuanxiaz.com/flsesim/">FLSeSim</a> was accepted by CVPR2021.</font></li>
              <li class="font">2019.03: <a target="_blank" href="https://chuanxiaz.com/pluralistic/">Pluralistic Image Completion</a> was accepted by CVPR2019.</font></li>
              <li class="font">2018.07: <a target="_blank" href='https://chuanxiaz.com/synthetic2real'>Synthetic2Real</a> was accepted by ECCV2018.</font></li>
              </ul>
              </div>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                His research interests are broadly in artificial intelligence, with emphasis on computer vision and machine learning. Much of his research is about 2D image generation, completion and translation, 3D scene reconstruction, generation and completion with the goal of building intelligent machines, capable of <b> rebuilding a photorealistic virtual world </b> via generative AI. In particular, he is working on topics:
                <li class="font">3D reconstruction and generation from limited views or videos.</font></li>
                <li class="font">3D editing (decomposition, recomposition, and completion) via object-centric representation.</font></li>
                <li class="font">Generative models for pyhsical world understanding.</font></li>
                <li class="font">Multi-modalities (1D, 2D, 3D, and 4D) generation and understanding.</font></li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="ipo_stop()" onmouseover="ipo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ipo_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/ipo_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/ipo_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function ipo_start() {
                  document.getElementById('ipo_image').style.opacity = "1";
                }

                function ipo_stop() {
                  document.getElementById('ipo_image').style.opacity = "0";
                }
                ipo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sm0kywu.github.io/ipoldm/">
                <papertitle>IPO-LDM: Depth-aided 360-degree Indoor RGB Panorama Outpainting via Latent Diffusion Model</papertitle>
              </a>
              <br>
              <a href="https://sm0kywu.github.io/CV/CV.html">Tianhao Wu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://sm0kywu.github.io/ipoldm/">project page</a> /
              <a href="">PDF</a> /
              <a href="https://arxiv.org/abs/2307.03177">arXiv</a> /
              <!-- <a href="https://www.youtube.com/watch?v=cy0fDO3-QSA">video</a> / -->
              <a href="https://sm0kywu.github.io/ipoldm/">code</a>
              <p></p>
              <p>An indoor panorama outpainting model using latent diffusion models with view-consistent.</p>
            </td>
          </tr>


          <tr onmouseout="cocktail_stop()" onmouseover="cocktail_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cocktail_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/cocktail_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <!-- <source src="images/cocktail_teaser.png" type="video/png"> -->
                <img src='images/cocktail_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function cocktail_start() {
                  document.getElementById('cocktail_image').style.opacity = "1";
                }

                function cocktail_stop() {
                  document.getElementById('cocktail_image').style.opacity = "0";
                }
                cocktail_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://mhh0318.github.io/cocktail/">
                <papertitle>Cocktail🍸: Mixing Multi-Modality Controls for Text-Conditional Image Generation</papertitle>
              </a>
              <br>
              <a href="https://mhh0318.github.io/">Minghui Hu</a>,
              <a href="https://github.com/jabir-zheng">Jianbin Zheng</a>,
              <a href="https://daqingliu.github.io/">Daqing Liu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://wang-chaoyue.github.io/">Chaoyue Wang</a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://mhh0318.github.io/cocktail/">project page</a> /
              <a href="">PDF</a> /
              <a href="https://arxiv.org/abs/2306.00964">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=cy0fDO3-QSA">video</a> /
              <a href="https://github.com/mhh0318/Cocktail">code</a>
              <p></p>
              <p>We develop a generalized framework for multi-modality control based on text-to-image generation.</p>
            </td>
          </tr>

          <tr onmouseout="matchnerf_stop()" onmouseover="matchnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='matchnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/matchnerf_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/matchnerf_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function matchnerf_start() {
                  document.getElementById('matchnerf_image').style.opacity = "1";
                }

                function matchnerf_stop() {
                  document.getElementById('matchnerf_image').style.opacity = "0";
                }
                matchnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://donydchen.github.io/matchnerf/">
                <papertitle>Explicit Correspondence Matching for Generalizable Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <a href="https://haofeixu.github.io/">Haofei Xu</a>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://donydchen.github.io/matchnerf/">project page</a> /
              <a href="https://arxiv.org/abs/2304.12294">arXiv</a> /
              <a href="https://github.com/donydchen/matchnerf">code</a>
              <p></p>
              <p>
                Employing explicit correspondence matching as a geometry prior enables NeRF to generalize across scenes.
              </p>
            </td>
          </tr>


          <tr onmouseout="cvq_stop()" onmouseover="cvq_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvq_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/cvq_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/cvq_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function cvq_start() {
                  document.getElementById('cvq_image').style.opacity = "1";
                }

                function cvq_stop() {
                  document.getElementById('cvq_image').style.opacity = "0";
                }
                cvq_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/cvq">
                <papertitle>Online clustered codebook</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://chuanxiaz.com/cvq">project page</a> /
              <a href="">PDF</a> /
              <a href="https://arxiv.org/abs/2307.15139">arXiv</a> /
              <!-- <a href="https://www.youtube.com/watch?v=cy0fDO3-QSA">video</a> / -->
              <a href="https://github.com/lyndonzheng/CVQ-VAE">code</a>
              <p></p>
              <p>A simple approach to avoid codebook collapse and achive 100% codebook utilisation.</p>
            </td>
          </tr>


          <tr onmouseout="VQWAE_stop()" onmouseover="VQWAE_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='VQWAE_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/UniD3_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/VQWAE_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function VQWAE_start() {
                  document.getElementById('VQWAE_image').style.opacity = "1";
                }

                function VQWAE_stop() {
                  document.getElementById('VQWAE_image').style.opacity = "0";
                }
                VQWAE_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://mhh0318.github.io/unid3/">
                <papertitle>Vector Quantized Wasserstein Auto-Encoder</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=DCC657sAAAAJ&hl=en">Long Tung Vuong</a>,
              <a href="https://scholar.google.com/citations?user=gysdMxwAAAAJ&hl=en/">Trung Le</a>,
              <a href="https://hezgit.github.io/">He zhao</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://sites.google.com/site/mehrtashharandi/">Mehrtash Harandi</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a>
              <br>
              <em>ICML</em>, 2023
              <br>
              <!-- <a href="https://mhh0318.github.io/unid3/">project page</a> / -->
              <a href="https://arxiv.org/abs/2302.05917">arXiv</a> /
              <a href="data/2023ICML_poster_vqwae.png">poster</a> /
              <a href="">code (coming soon)</a>
              <p></p>
              <p>Minimize the codebook-data distortion as the Wasserstein distance.</p>
            </td>
          </tr>

          <tr onmouseout="UniD3_stop()" onmouseover="UniD3_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='UniD3_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/UniD3_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/UniD3_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function UniD3_start() {
                  document.getElementById('UniD3_image').style.opacity = "1";
                }

                function UniD3_stop() {
                  document.getElementById('UniD3_image').style.opacity = "0";
                }
                UniD3_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://mhh0318.github.io/unid3/">
                <papertitle>UniD3: Unified Discrete Diffusion for Simultaneous Vision-Language Generation</papertitle>
              </a>
              <br>
              <a href="https://mhh0318.github.io/">Minghui Hu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://scholar.google.com/citations?user=VRgciTQAAAAJ&hl">Heliang Zheng</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://wang-chaoyue.github.io/">Chaoyue Wang</a>,
              <a href="https://dblp.org/pid/254/9451.html">Zuopeng Yang</a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a>,
              <a href="https://www3.ntu.edu.sg/home/epnsugan/">P.N.Suganthan</a>,
              <br>
              <em>ICLR</em>, 2023
              <br>
              <a href="https://mhh0318.github.io/unid3/">project page</a> /
              <a href="https://arxiv.org/abs/2211.14842">arXiv</a> /
              <a href="https://github.com/mhh0318/UniD3">code</a> /
              <p></p>
              <p>A unified discrete diffusion model for simultaneous vision-language generation.</p>
            </td>
          </tr>

          <tr onmouseout="movq_stop()" onmouseover="movq_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='movq_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/movq_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/movq_teaser.jpg' width="160">
              </div>
              <script type="text/javascript">
                function movq_start() {
                  document.getElementById('movq_image').style.opacity = "1";
                }

                function movq_stop() {
                  document.getElementById('movq_image').style.opacity = "0";
                }
                movq_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/movq">
                <papertitle>MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://scholar.google.com/citations?user=DCC657sAAAAJ&hl=en"> Long Tung Vuong</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a>
              <br>
              <em>NeurIPS (Spotlight)</em>, 2022
              <br>
              <a href="https://chuanxiaz.com/movq">project page</a> /
              <a href="https://openreview.net/pdf?id=Qb-AoSw4Jnm">PDF</a> /
              <a href="https://arxiv.org/abs/2209.09002">arXiv</a> /
              <a href="https://recorder-v3.slideslive.com/?share=75742&s=46fd4642-e778-4156-bc73-151f85156f30">video</a> /
              <a href="https://github.com/ai-forever/Kandinsky-2/tree/main/kandinsky2/vqgan">code(Kandinsky2)</a> /
              <a href="data/2022NeurIPS_poster_MOVQ.png">poster</a> /
              <p></p>
              <p>A spatially conditional normalization is introduced to address the repeated artifacts in vector quantized methods.</p>
            </td>
          </tr>

          <tr onmouseout="objectsdf_stop()" onmouseover="objectsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='objectsdf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/objectsdf_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/objectsdf_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function objectsdf_start() {
                  document.getElementById('objectsdf_image').style.opacity = "1";
                }

                function objectsdf_stop() {
                  document.getElementById('objectsdf_image').style.opacity = "0";
                }
                objectsdf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://wuqianyi.top/objectsdf/">
                <papertitle>Object-Compositional Neural Implicit Surfaces</papertitle>
              </a>
              <br>
              <a href="https://wuqianyi.top/"> Qianyi Wu</a>,
              <a href="https://alvinliu0.github.io/"> Xian Liu</a>,
              <a href="https://donydchen.github.io/"> Yuedong Chen</a>,
              <a href="https://likojack.github.io/kejieli/#/home"> Kejie Li</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://personal.ntu.edu.sg/asjmzheng/">Jianmin Zheng</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://wuqianyi.top/objectsdf/">project page</a> /
              <a href="https://arxiv.org/abs/2207.09686">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=23vxOV19bEw">video</a> /
              <a href="https://github.com/QianyiWu/objsdf">code</a>
              <p></p>
              <p>Automatically decompose a scene into 3D instance, trained using only 2D semantic lables and images.</p>
            </td>
          </tr>

          <tr onmouseout="semantic2NeRF_stop()" onmouseover="semantic2NeRF_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='semantic2NeRF_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/semantic2NeRF_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/semantic2NeRF_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function semantic2NeRF_start() {
                  document.getElementById('semantic2NeRF_image').style.opacity = "1";
                }

                function semantic2NeRF_stop() {
                  document.getElementById('semantic2NeRF_image').style.opacity = "0";
                }
                semantic2NeRF_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://donydchen.github.io/sem2nerf/">
                <papertitle>Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/"> Yuedong Chen</a>,
              <a href="https://wuqianyi.top/"> Qianyi Wu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://donydchen.github.io/sem2nerf/">project page</a> /
              <a href="https://arxiv.org/abs/2203.10821">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=cYr3Dz8N_9E&feature=youtu.be">video</a> /
              <a href="https://github.com/donydchen/sem2nerf">code</a>
              <p></p>
              <p>We train a 3D inversion model to transfer the 2D semantic map into 3D NeRF, and lets users edit 3D model through 2D semantic input.</p>
            </td>
          </tr>

          <tr onmouseout="tfill_stop()" onmouseover="tfill_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tfill_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/tfill_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/tfill_teaser.jpg' width="160">
              </div>
              <script type="text/javascript">
                function tfill_start() {
                  document.getElementById('tfill_image').style.opacity = "1";
                }

                function tfill_stop() {
                  document.getElementById('tfill_image').style.opacity = "0";
                }
                tfill_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://chuanxiaz.com/tfill">
                <papertitle>Bridging global context interactions for high-fidelity image completion</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
							<a href="https://chuanxiaz.com/tfill">project page</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Bridging_Global_Context_Interactions_for_High-Fidelity_Image_Completion_CVPR_2022_paper.pdf">PDF</a> /
							<a href="https://arxiv.org/abs/2104.00845">arXiv</a> /
							<a href="https://youtu.be/efB1fw0jiLs">video</a> /
							<a href="https://github.com/lyndonzheng/TFill">code</a> /
              <a href="data/2022CVPR_poster_TFill.pdf">poster</a>
              <p></p>
              <p>TFill fills in reasonable contents for both foreground object removal and content completion.</p>
            </td>
          </tr>

          <tr onmouseout="vinv_stop()" onmouseover="vinv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vinv_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vinv_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vinv_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function vinv_start() {
                  document.getElementById('vinv_image').style.opacity = "1";
                }

                function vinv_stop() {
                  document.getElementById('vinv_image').style.opacity = "0";
                }
                vinv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/vinv">
                <papertitle>Visiting the Invisible: Layer-by-Layer Completed Scene Decomposition</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://scholar.google.com/citations?user=AZWFq1sAAAAJ&hl=en">Duy-Son Dao</a>,
              <a href="https://guoxiansong.github.io/homepage/index.html">Guoxian Song</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <br>
              <em>IJCV</em>, 2021
              <br>
              <a href="https://chuanxiaz.com/vinv">project page</a> /
              <a href="https://link.springer.com/article/10.1007/s11263-021-01517-0">PDF</a> /
              <a href="https://arxiv.org/abs/2104.05367">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=QSAYxrKgn7A">video</a> /
              <a href="https://github.com/lyndonzheng/VINV">code</a>
              <p></p>
              <p>We build a high-level scene understanding system that simultaneously models the completed shape and appearance for all instances.</p>
            </td>
          </tr>


          <tr onmouseout="agilegan_stop()" onmouseover="agilegan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='agilegan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/agilegan_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/agilegan_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function agilegan_start() {
                  document.getElementById('agilegan_image').style.opacity = "1";
                }

                function agilegan_stop() {
                  document.getElementById('agilegan_image').style.opacity = "0";
                }
                agilegan_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://guoxiansong.github.io/homepage/agilegan.html">
                <papertitle>AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning</papertitle>
              </a>
              <br>
              <a href="https://guoxiansong.github.io/homepage/index.html">Guoxian Song</a>,
              <a href="http://linjieluo.com/">Linjie Luo</a>,
              <a href="https://www.jingliu.net/">Jing Liu</a>,
              <a href="https://en.wikipedia.org/wiki/Alex_Ma">Wan-Chun Ma</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <br>
              <em>SIGGRAPH</em>, 2021
              <br>
              <a href="https://guoxiansong.github.io/homepage/agilegan.html">project page</a> /
              <a href="https://guoxiansong.github.io/homepage/paper/AgileGAN.pdf">PDF</a> /
              <a href="https://www.youtube.com/embed/o0HHIvOT_n0">video</a> /
              <a href="https://github.com/GuoxianSong/AgileGAN">code</a> /
              <a href="http://www.agilegan.com/">Online Demo</a>
              <p></p>
              <p>A GAN inversion model is trained for Stylizing Portraits.</p>
            </td>
          </tr>


          <tr onmouseout="flsesim_stop()" onmouseover="flsesim_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='flsesim_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/flsesim_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/flsesim_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function flsesim_start() {
                  document.getElementById('flsesim_image').style.opacity = "1";
                }
                function flsesim_stop() {
                  document.getElementById('flsesim_image').style.opacity = "0";
                }
                flsesim_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/flsesim">
                <papertitle>The Spatially-Correlative Loss for Various Image Translation Tasks</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://chuanxiaz.com/flsesim">project page</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_The_Spatially-Correlative_Loss_for_Various_Image_Translation_Tasks_CVPR_2021_paper.pdf">PDF</a> /
              <a href="https://arxiv.org/abs/2104.00854">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=pu6PT1om2r0">video</a> /
              <a href="https://github.com/lyndonzheng/F-LSeSim">code</a> /
              <a href="data/2022CVPR_poster_TFill.pdf">poster</a>
              <p></p>
              <p>We propose a novel spatially-correlative loss that is simple, efficient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired I2I translation.</p>
            </td>
          </tr>


          <tr onmouseout="pic_stop()" onmouseover="pic_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pic_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/pic_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/pic_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function pic_start() {
                  document.getElementById('pic_image').style.opacity = "1";
                }
                function pic_stop() {
                  document.getElementById('pic_image').style.opacity = "0";
                }
                pic_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/pic">
                <papertitle>Pluralistic (Free-Form) Image Completion</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>IJCV</em>, 2021
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="https://chuanxiaz.com/pic">project page</a> /
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Pluralistic_Image_Completion_CVPR_2019_paper.pdf">PDF</a> /
              <a href="https://arxiv.org/abs/1903.04227">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs">video</a> /
              <a href="https://github.com/lyndonzheng/Pluralistic-Inpainting">code</a> /
              <a href="data/poster_picnet_cvpr19.pdf">poster</a>
              <p></p>
              <p>Given a single masked image, the proposed model is able to generate multiple and diverse plausible results.</p>
            </td>
          </tr>


          <tr onmouseout="synthetic2real_stop()" onmouseover="synthetic2real_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='synthetic2real_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/synthetic2real_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/synthetic2real_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function synthetic2real_start() {
                  document.getElementById('synthetic2real_image').style.opacity = "1";
                }
                function synthetic2real_stop() {
                  document.getElementById('synthetic2real_image').style.opacity = "0";
                }
                synthetic2real_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/synthetic2real">
                <papertitle>T2Net: Synthetic-to-Realistic Translation for Depth Estimation Tasks</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="https://chuanxiaz.com/synthetic2real">project page</a> /
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chuanxia_Zheng_T2Net_Synthetic-to-Realistic_Translation_ECCV_2018_paper.pdf">PDF</a> /
              <a href="https://arxiv.org/abs/1808.01454">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=B6lOToIk0xY">video</a> /
              <a href="https://github.com/lyndonzheng/Synthetic2Realistic">code</a> /
              <a href="data/poster_syn2real_eccv18.pdf">poster</a>
              <p></p>
              <p>Without any real depth map, the proposed model evaluates depth maps on real scenes using only synthetic datasets.</p>
            </td>
          </tr>

    </tbody></table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <heading>Academic Services</heading>
          <p><font size="4"><b>Conference Reviewer</b></font></p>
        <table>
          <tr>
            <td>CVPR</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2020,&nbsp;2021,&nbsp;2022,&nbsp;2023 (Outstanding Reviewer)</td>
          </tr>
          <tr><td>ICCV</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2019,&nbsp;2021,&nbsp;2023</td></tr>
          <tr><td>ECCV</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2020,&nbsp;2022</td></tr>
          <tr><td>NeurIPS</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022,&nbsp;2023</td></tr>
          <tr><td>ICLR</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2021,&nbsp;2022,&nbsp;2023</td></tr>
          <tr><td>ICML</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2023</td></tr>
          <tr><td>SIGGRAPH&Asia</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>ICRA</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>IROS</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>IJCAI</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>ACM MM</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2021,&nbsp;2022</td></tr>
        </table>
          <p><font size="4"><b>Journal Reviewer</b></font></p>
          <p>TPAMI, IJCV, TIP, JAS, TMM(Outstanding Reviewer Award, 2021), TCSVT, CVIU, TVCJ, NCAA</p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Teaching</heading>
          <ul style="text-align:justify;height: 00px;">
              <li class="font">Teaching, Generative AI, Graduate, Oxford Summer School, 2023</font></li>
              <li class="font">Teaching Assistant, Advanced Digital Image Processing, Graduate, NTU, 2018-2020</font></li>
              <li class="font">Teaching Assistant, Human-Computer Interaction, Undergraduate, NTU, 2018-2020</font></li>
              <li class="font">Teaching Assistant, Engineering Mathematics, Undergraduate, NTU, 2018-2020</font></li>
          </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
              <a href="https://jonbarron.info">awesome website template </a><br>
              Last updated July 2023.
          </font>
            </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <script type="text/javascript" src="//ra.revolvermaps.com/0/0/1.js?i=0s5f0y0jjk8&amp;s=180&amp;m=6&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
            </td>
            </tr>
            </tbody></table>

      </td>
    </tr>
  </table>

  </body>
</html>

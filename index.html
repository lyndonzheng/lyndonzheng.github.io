<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chuanxia Zheng</title>

  <meta name="author" content="Chuanxia Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <!-- Bio -->
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chuanxia Zheng</name>
              </p>
              <p> <a href="https://eng.ox.ac.uk/people/chuanxia-zheng/"> Chuanxia Zheng</a> 
                is a <a href="https://marie-sklodowska-curie-actions.ec.europa.eu/actions/postdoctoral-fellowships">Marie Sk≈Çodowska-Curie Actions (MSCA) Fellow</a> and a postdoctoral researcher in <a href="https://www.robots.ox.ac.uk/~vgg/"> VGG</a> 
                at the <a href="https://www.ox.ac.uk/">University of Oxford</a>, 
                working with Prof. <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> 
                on 3D reconstruction and generation and 
                Prof. <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a> on generative model for understanding.
              </p>
              <p>
                Before that, he spent one year at <a href="https://www.monash.edu/">Monash University</a>, where he worked as a Research Fellow with Prof. <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> and Prof. <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a> on codebook learning for generation. He received his PhD degree from the <a href="https://www.ntu.edu.sg/scse"> SCSE </a> at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>, supervised by Prof. <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a> and Prof. <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> on 2D generation, translation and completion. His thesis <a href="https://arxiv.org/abs/2202.12752">Synthesizing Photorealistic Images</a> was awarded the <a href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-outstanding-phd-thesis-award-2022"> NTU Outstanding PhD Thesis Award 2022</a>. <!--Before that, I received the Master‚Äôs degree in the <a href="http://irmct.buaa.edu.cn/"> IR&MCT Lab </a> at Beihang University, advised by Jianhua Wang and <a href="http://dept3.buaa.edu.cn/szjs/zzjs/dgdzjxsyzx1/js/cwh.htm"> Weihai Chen</a> -->
              </p>
              <p style="text-align:center">
                <a href="CHUANXIA001@e.ntu.edu.sg">Email</a> &nbsp/&nbsp
                <a href="data/cv_chuanxia.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mvpE6bIAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/lyndonzheng">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/ChuanxiaZ">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <div class="news">
              <ul style="text-align:justify;height: 150px;">
              <li class="font"><font color="red">[2024.05:] <a target="_blank" href="https://chuanxiaz.com/picformer/">PICFormer</a> is finally accepted by T-PAMI</font></li>
              <li class="font"><font color="red">[2024.05:] Invited as Area Chair for BMVC2024</font></li>
              <li class="font">[2024.02:] Happy to be awarded <a target="_blank" href="https://marie-sklodowska-curie-actions.ec.europa.eu/actions/postdoctoral-fellowships">Marie Sk≈Çodowska-Curie Actions (MSCA) Fellow</a>.</li> 
              <li class="font">[2024.02:] <a target="_blank" href="https://chuanxiaz.com/free3d/">Free3D</a>,
                <a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/research/amodal/">Amodal in the wild</a>,
                and <a target="_blank" href="https://jabir-zheng.github.io/OneMoreStep/">OneMoreStep</a> are accepted by CVPR2024.
                Congrats to <a target="_blank" href="https://www.robots.ox.ac.uk/~guanqi/">Guanqi</a>, and
                <a target="_blank" href="https://mhh0318.github.io/">Minghui</a>.</li>   
              <li class="font">[2024.06:] Call for Papers. We are organizing the <a target="_blank" href="https://abdullahamdi.com/3dmv2024/">2th Learning 3D with Multi-View Supervision</a> at CVPR2024.</li>
              <li class="font">[2024.01:] Invited as Area Chair for ACMMM2024.</li>
              <li class="font">[2024.01:] <a target="_blank" href="https://sm0kywu.github.io/panodiffusion/">Panodiffusion</a> is accepted by ICLR2024. Congrats to <a target="_blank" href="https://sm0kywu.github.io/CV/CV.html">Tianhao</a>.</li>
              <li class="font">[2023.09:] <a target="_blank" href="https://mhh0318.github.io/cocktail/">Cocktailüç∏</a> is accepted by NeurIPS2023.</li>
              <li class="font">[2023.07:] <a target="_blank" href="https://chuanxiaz.com/cvq">CVQ-VAE</a> is accepted by ICCV2023.</li>
              <li class="font">[2023.05:] Recognized as an <a target="_blank" href="https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers"> Outstanding Reviewer</a> at CVPR2023!</a></li>
              <li class="font">[2023.04:] <a target="_blank" href="https://chuanxiaz.com/movq">MoVQ</a> is reported means a lot to <a target="_blank" href="https://habr.com/ru/companies/sberbank/articles/725282/">Kandinsky2.1.</a>, <a target="_blank" href="https://github.com/ai-forever/Kandinsky-2">Github.</a></li>
              <li class="font">[2023.04:] <a target="_blank" href="https://chuanxiaz.com">VQ-WAE</a> is accepted by ICML2023.</li>
              <li class="font">[2023.01:] <a target="_blank" href="https://mhh0318.github.io/unid3/">UniD3</a> is accepted by ICLR2023.</li>
              <li class="font">[2022.12:] Started the research in <a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/">VGG</a> at University of Oxford.</li>
              <li class="font">[2022.09:] <a target="_blank" href="https://chuanxiaz.com/movq">MoVQ</a> is accepted by NeurIPS2022.</li>
              <li class="font">[2022.08:] <a target="_blank" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0271056">Nuclear Predication</a> is accepted by PLOS ONE.</li>
              <li class="font">[2022.07:] <a target="_blank" href="https://wuqianyi.top/objectsdf/">ObjectSDF</a> and <a target="_blank" href="https://donydchen.github.io/sem2nerf/">Semantic2NeRF</a> were accepted by ECCV2022.</li>
              <li class="font">[2022.06:] Happy to be the winner of the <a target="_blank" href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-outstanding-phd-thesis-award-2022">NTU Outstanding PhD Thesis Award 2022</a>.</li>
              <li class="font">[2022.03:] <a target="_blank" href="https://mp.weixin.qq.com/s/e1gqtDsH872sjfH4T4oL2w?scene=25#wechat_redirect">Synthesizing Photorealistic Scenes</a> is reported on media.</li>
              <li class="font">[2022.03:] <a target="_blank" href="https://chuanxiaz.com/tfill">TFill</a> is accepted by CVPR2022.</li>
              <li class="font">[2021.08:] <a target="_blank" href="https://chuanxiaz.com/vinv">Visiting the Invisible</a> is accepted by IJCV.</li>
              <li class="font">[2021.07:] One paper is accepted by ICCV2021.</li>
              <li class="font">[2021.06:] Recognized as <a target="_blank" href="data/TMM_ORA.pdf"> Outstanding Reviewer</a> at TMM2021!</a></li>
              <li class="font">[2021.06:] <a target="_blank" href="https://link.springer.com/article/10.1007/s11263-021-01502-7">Pluralistic Free-Form Image Completion </a> is accepted by IJCV.</li>
              <li class="font">[2021.06:] <a target="_blank" href="https://guoxiansong.github.io/homepage/agilegan.html">AgileGAN</a> is reported by <a target="_blank" href="https://www.sohu.com/a/471951100_629135">Sohu</a> and reproduced by <a target="_blank" href="https://github.com/open-mmlab/MMGEN-FaceStylor">Open MMLab</a>.</li>
              <li class="font">[2021.04:] <a target="_blank" href="https://guoxiansong.github.io/homepage/agilegan.html">AgileGAN</a> is accepted by SIGGRAPH2021.</li>
              <li class="font">[2021.03:] <a target="_blank" href="https://chuanxiaz.com/flsesim/">FLSeSim</a> is accepted by CVPR2021.</li>
              <li class="font">[2019.03:] <a target="_blank" href="https://chuanxiaz.com/pluralistic/">Pluralistic Image Completion</a> is accepted by CVPR2019.</li>
              <li class="font">[2018.07:] <a target="_blank" href='https://chuanxiaz.com/synthetic2real'>Synthetic2Real</a> is accepted by ECCV2018.</li>
              </ul>
              </div>
            </td>
          </tr>
        </tbody></table>

        <!-- Research -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                His research interests focus on computer vision and machine learning, especially for creative AI.
                He has done a wide range of work on 2D and 3D scene synthesis, with the goal of <b>synthesizing a photorealistic physical world</b> via generative AI. 
                In particular, on topics:
                <ul style="text-align:justify;height: 150px;">
                  <li class="font">3D geometry and appearance synthesis from limited views or videos.</li>
                  <li class="font">3D editing via object-centric perception.</li>
                  <li class="font">Generative models for physical underlying and interaction.</li>
                  <li class="font">Multi-modalities (1D, 2D, 3D, and 4D) generation.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- DragAPart -->
          <tr onmouseout="DragAPart_stop()" onmouseover="DragAPart_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DragAPart_image'>
                <img src='images/DragAPart_teaser_o.png' width="160">
                </div>
                <img src='images/DragAPart_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function DragAPart_start() {
                  document.getElementById('DragAPart_image').style.opacity = "1";
                }
    
                function DragAPart_stop() {
                  document.getElementById('DragAPart_image').style.opacity = "0";
                }
                DragAPart_stop()
              </script>
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
            <a href="https://dragapart.github.io/">
              <papertitle>DragAPart: Learning a Part-Level Motion Prior for Articulated Objects
              </papertitle>
            </a>
            <br>
            <a href="https://ruiningli.com/">Ruining Li</a>,
            <strong>Chuanxia Zheng</strong>,
            <a href="https://chrirupp.github.io/">Christian Rupprecht</a>,
            <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
            <br>
            <em>arXiv</em>, 2024 
            <br>
            <a href="https://dragapart.github.io/">project page</a> /
            <a href="https://arxiv.org/abs/2403.15382">arXiv</a> /
            <a href="https://github.com/RuiningLi/DragAPart">code</a>/
            <a href="https://huggingface.co/spaces/rayli/DragAPart">demo</a>
            <p></p>
            <p>
              A physical interaction with objects in vision for part-level dragging.
            </p>
          </td>
        </tr>

          <!-- MVSplat -->
          <tr>
            <tr onmouseout="mvsplat_stop()" onmouseover="mvsplat_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mvsplat_image'>
                  <video  width=100% height=100% muted autoplay loop>
                  <source src="images/mvsplat_teaser.mp4" type="video/mp4">
                  </video></div>
                  <img src='images/mvsplat_teaser.png' width="160">
                </div>
                <script type="text/javascript">
                  function mvsplat_start() {
                    document.getElementById('mvsplat_image').style.opacity = "1";
                  }
      
                  function mvsplat_stop() {
                    document.getElementById('mvsplat_image').style.opacity = "0";
                  }
                  mvsplat_stop()
                </script>
              </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://donydchen.github.io/mvsplat/">
                <papertitle>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images
                </papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <a href="https://haofeixu.github.io/">Haofei Xu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>arXiv</em>, 2024 
              <br>
              <a href="https://donydchen.github.io/mvsplat/">project page</a> /
              <a href="https://arxiv.org/abs/2403.14627">arXiv</a> /
              <a href="https://github.com/donydchen/mvsplat">code</a>
              <p></p>
              <p>
                A cost volume representation for efficiently predicting 3D Gaussians from sparse multi-view images in a single forward pass.
              </p>
            </td>
          </tr>

          <!-- clusteringsdf -->
          <tr onmouseout="clusteringsdf_stop()" onmouseover="clusteringsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='clusteringsdf_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/clusteringsdf_teaser.mp4" type="video/mp4">
                </video></div>
                <img src='images/clusteringsdf_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function clusteringsdf_start() {
                  document.getElementById('clusteringsdf_image').style.opacity = "1";
                }
    
                function clusteringsdf_stop() {
                  document.getElementById('clusteringsdf_image').style.opacity = "0";
                }
                clusteringsdf_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://sm0kywu.github.io/ClusteringSDF">
                ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition
              </a>
              <br>
              <a href="https://sm0kywu.github.io/CV/CV.html">Tianhao Wu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://qianyiwu.github.io/">Qianyi Wu</a>
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://sm0kywu.github.io/ClusteringSDF">project page</a> /
              <a href="https://arxiv.org/abs/2403.14619">arXiv</a> /
              <a href="">video</a> /
              <a href="">code</a>
              <p></p>
              <p>A self-organized 3D segmentation model via neural implicit surface representation.</p>
            </td>
          </tr>


          <!-- matchnerf -->
          <tr onmouseout="matchnerf_stop()" onmouseover="matchnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='matchnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/matchnerf_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/matchnerf_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function matchnerf_start() {
                  document.getElementById('matchnerf_image').style.opacity = "1";
                }

                function matchnerf_stop() {
                  document.getElementById('matchnerf_image').style.opacity = "0";
                }
                matchnerf_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://donydchen.github.io/matchnerf/">
                <papertitle>Explicit Correspondence Matching for Generalizable Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <a href="https://haofeixu.github.io/">Haofei Xu</a>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://donydchen.github.io/matchnerf/">project page</a> /
              <a href="https://arxiv.org/abs/2304.12294">arXiv</a> /
              <a href="https://github.com/donydchen/matchnerf">code</a>
              <p></p>
              <p>
                Employing explicit correspondence matching as a geometry prior enables NeRF to generalize across scenes.
              </p>
            </td>
          </tr>

          <!-- PICFormer -->
          <tr onmouseout="picformer_stop()" onmouseover="picformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='picformer_image'>
                <video  padding="20" width=100% height=100% muted autoplay loop>
                <source src="images/picformer_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/picformer_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function picformer_start() {
                  document.getElementById('picformer_image').style.opacity = "1";
                }

                function picformer_stop() {
                  document.getElementById('picformer_image').style.opacity = "0";
                }
                picformer_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/picformer">
                <papertitle>Bridging Global Context Interactions for High-Fidelity Pluralistic Image Completion
                </papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://guoxiansong.github.io/homepage/index.html">Guoxian Song</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="http://linjieluo.com/">Linjie Luo</a>,
              <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a>
              <br>
              <em>T-PAMI</em>, 2024
              <br>
              <a href="https://chuanxiaz.com/picformer">project page</a> /
              <a href="picformer/static/images/TPAMI2024_PICFormer.pdf">PDF</a> /
              <a href="https://ieeexplore.ieee.org/document/10535740">ieeexplore</a> /
              <a href="https://youtu.be/yRBzGMmS0Ew">video</a> /
              <a href="https://github.com/lyndonzheng/TFill">code</a> /
              <p></p>
              <p><strong>PICFormer</strong> achieves pluralistic image completion with multiple and diverse
                solutions using a transformer based architecture at a much faster inference speed.</p>
            </td>
          </tr>

          <!-- free3d -->
          <tr onmouseout="free3d_stop()" onmouseover="free3d_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='free3d_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/free3d_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/free3d_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function free3d_start() {
                  document.getElementById('free3d_image').style.opacity = "1";
                }

                function free3d_stop() {
                  document.getElementById('free3d_image').style.opacity = "0";
                }
                free3d_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/free3d">
                <papertitle>Free3D: Consistent Novel View Synthesis without 3D Representation
                </papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://chuanxiaz.com/free3d">project page</a> /
              <a href="free3d/static/images/Free3D.pdf">PDF</a> /
              <a href="https://arxiv.org/pdf/2312.04551.pdf">arXiv</a> /
              <a href="https://youtu.be/7CdYuZ7D1DY">video</a> /
              <a href="https://github.com/lyndonzheng/Free3D/">code</a> /
              <!-- <a href="data/2023ICCV_poster_CVQ.pdf">poster</a> -->
              <p></p>
              <p><strong>Free3D</strong> synthesizes consistent novel views on open-set categories 
                without the need of explicit 3D representations.</p>
            </td>
          </tr>

          <!--amodal -->
          <tr onmouseout="amodal_stop()" onmouseover="amodal_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='amodal_image' style='opacity:0;'>
                <img src='images/amodal_teaser.png' padding="20" width="160"></div>
                <img src='images/amodal_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function amodal_start() {
                  document.getElementById('amodal_image').style.opacity = "1";
                }

                function amodal_stop() {
                  document.getElementById('amodal_image').style.opacity = "0";
                }
                amodal_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://www.robots.ox.ac.uk/~vgg/research/amodal/">
                <papertitle>Amodal Ground Truth and Completion in the Wild
                </papertitle>
              </a>
              <br>
              <a href="https://www.robots.ox.ac.uk/~guanqi/">Guanqi Zhan</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/amodal/">project page</a> /
              <a href="https://arxiv.org/pdf/2312.17247.pdf">PDF</a> /
              <a href="https://arxiv.org/pdf/2312.17247.pdf">arXiv</a> /
              <!-- <a href="https://youtu.be/7CdYuZ7D1DY">video</a> / -->
              <a href="https://github.com/Championchess/Amodal-Completion-in-the-Wild">code</a> /
              <!-- <a href="data/2023ICCV_poster_CVQ.pdf">poster</a> -->
              <p></p>
              <p>Setting up a Stable Diffusion based network to 
                solve the amodal completion problem for any category and without occluder mask provided.</p>
            </td>
          </tr>

          <!-- OMS -->
          <tr onmouseout="oms_stop()" onmouseover="oms_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='oms_image' style='opacity:0;'>
                <img src='images/oms_0.png' padding="20" width="160"></div>
                <img src='images/oms_1.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function oms_start() {
                  document.getElementById('oms_image').style.opacity = "1";
                }

                function oms_stop() {
                  document.getElementById('oms_image').style.opacity = "0";
                }
                oms_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://jabir-zheng.github.io/OneMoreStep/">
                <papertitle>One More Step: A Versatile Plug-and-Play Module for 
                  Rectifying Diffusion Schedule Flaws and Enhancing Low-Frequency Controls</papertitle>
              </a>
              <br>
              <a href="https://mhh0318.github.io/">Minghui Hu</a>,
              <a href="https://github.com/jabir-zheng">Jianbin Zheng</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://wang-chaoyue.github.io/">Chaoyue Wang</a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://jabir-zheng.github.io/OneMoreStep/">project page</a> /
              <a href="http://arxiv.org/abs/2311.15744">PDF</a> /
              <a href="http://arxiv.org/abs/2311.15744">arXiv</a> /
              <a href="https://github.com/mhh0318/OneMoreStep">code</a> /
              <a href="https://huggingface.co/spaces/h1t/oms_sdxl_lcm">HugeFace</a>
              <p></p>
              <p>A versatile plug-and-play module to fix the scheduler flaws for diffusion models.</p>
            </td>
          </tr>

          <!-- PanoDiffusion -->
          <tr onmouseout="ipo_stop()" onmouseover="ipo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ipo_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/ipo_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/ipo_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function ipo_start() {
                  document.getElementById('ipo_image').style.opacity = "1";
                }

                function ipo_stop() {
                  document.getElementById('ipo_image').style.opacity = "0";
                }
                ipo_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://sm0kywu.github.io/panodiffusion/">
                <papertitle>PanoDiffusion: 360-degree Panorama Outpainting via Diffusion</papertitle>
              </a>
              <br>
              <a href="https://sm0kywu.github.io/CV/CV.html">Tianhao Wu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://sm0kywu.github.io/panodiffusion/">project page</a> /
              <a href="">PDF</a> /
              <a href="https://arxiv.org/abs/2307.03177">arXiv</a> /
              <!-- <a href="https://www.youtube.com/watch?v=cy0fDO3-QSA">video</a> / -->
              <a href="https://sm0kywu.github.io/panodiffusion/">code</a>
              <p></p>
              <p>An indoor panorama outpainting model using latent diffusion models with view-consistent.</p>
            </td>
          </tr>

          <!-- cocktail -->
          <tr onmouseout="cocktail_stop()" onmouseover="cocktail_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cocktail_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/cocktail_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <!-- <source src="images/cocktail_teaser.png" type="video/png"> -->
                <img src='images/cocktail_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function cocktail_start() {
                  document.getElementById('cocktail_image').style.opacity = "1";
                }

                function cocktail_stop() {
                  document.getElementById('cocktail_image').style.opacity = "0";
                }
                cocktail_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://mhh0318.github.io/cocktail/">
                <papertitle>Cocktailüç∏: Mixing Multi-Modality Controls for Text-Conditional Image Generation</papertitle>
              </a>
              <br>
              <a href="https://mhh0318.github.io/">Minghui Hu</a>,
              <a href="https://github.com/jabir-zheng">Jianbin Zheng</a>,
              <a href="https://daqingliu.github.io/">Daqing Liu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://wang-chaoyue.github.io/">Chaoyue Wang</a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://mhh0318.github.io/cocktail/">project page</a> /
              <a href="">PDF</a> /
              <a href="https://arxiv.org/abs/2306.00964">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=cy0fDO3-QSA">video</a> /
              <a href="https://github.com/mhh0318/Cocktail">code</a>
              <p></p>
              <p>We develop a generalized framework for multi-modality control based on text-to-image generation.</p>
            </td>
          </tr>

          <!-- CVQ -->
          <tr onmouseout="cvq_stop()" onmouseover="cvq_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvq_image'>
                <video  width=100% height=100% muted autoplay loop>
                <source src="images/cvq_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/cvq_teaser.png' padding="20" width="160">
              </div>
              <script type="text/javascript">
                function cvq_start() {
                  document.getElementById('cvq_image').style.opacity = "1";
                }

                function cvq_stop() {
                  document.getElementById('cvq_image').style.opacity = "0";
                }
                cvq_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/cvq">
                <papertitle>Online clustered codebook</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://chuanxiaz.com/cvq">project page</a> /
              <a href="">PDF</a> /
              <a href="https://arxiv.org/abs/2307.15139">arXiv</a> /
              <a href="https://youtu.be/g098J5Obxvs">video</a> /
              <a href="https://github.com/lyndonzheng/CVQ-VAE">code</a> /
              <a href="data/2023ICCV_poster_CVQ.pdf">poster</a>
              <p></p>
              <p>A simple approach to avoid codebook collapse and achive 100% codebook utilisation.</p>
            </td>
          </tr>


          <!-- VQWAE -->
          <tr onmouseout="VQWAE_stop()" onmouseover="VQWAE_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='VQWAE_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/UniD3_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/VQWAE_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function VQWAE_start() {
                  document.getElementById('VQWAE_image').style.opacity = "1";
                }

                function VQWAE_stop() {
                  document.getElementById('VQWAE_image').style.opacity = "0";
                }
                VQWAE_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://mhh0318.github.io/unid3/">
                <papertitle>Vector Quantized Wasserstein Auto-Encoder</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=DCC657sAAAAJ&hl=en">Long Tung Vuong</a>,
              <a href="https://scholar.google.com/citations?user=gysdMxwAAAAJ&hl=en/">Trung Le</a>,
              <a href="https://hezgit.github.io/">He zhao</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://sites.google.com/site/mehrtashharandi/">Mehrtash Harandi</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a>
              <br>
              <em>ICML</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2302.05917">arXiv</a> /
              <a href="data/2023ICML_poster_vqwae.png">poster</a> /
              <p></p>
              <p>Minimize the codebook-data distortion as the Wasserstein distance.</p>
            </td>
          </tr>

          <!-- UniD3 -->
          <tr onmouseout="UniD3_stop()" onmouseover="UniD3_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='UniD3_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/UniD3_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/UniD3_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function UniD3_start() {
                  document.getElementById('UniD3_image').style.opacity = "1";
                }

                function UniD3_stop() {
                  document.getElementById('UniD3_image').style.opacity = "0";
                }
                UniD3_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://mhh0318.github.io/unid3/">
                <papertitle>UniD3: Unified Discrete Diffusion for Simultaneous Vision-Language Generation</papertitle>
              </a>
              <br>
              <a href="https://mhh0318.github.io/">Minghui Hu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://scholar.google.com/citations?user=VRgciTQAAAAJ&hl">Heliang Zheng</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://wang-chaoyue.github.io/">Chaoyue Wang</a>,
              <a href="https://dblp.org/pid/254/9451.html">Zuopeng Yang</a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a>,
              <a href="https://www3.ntu.edu.sg/home/epnsugan/">P.N.Suganthan</a>,
              <br>
              <em>ICLR</em>, 2023
              <br>
              <a href="https://mhh0318.github.io/unid3/">project page</a> /
              <a href="https://arxiv.org/abs/2211.14842">arXiv</a> /
              <a href="https://github.com/mhh0318/UniD3">code</a> /
              <p></p>
              <p>A unified discrete diffusion model for simultaneous vision-language generation.</p>
            </td>
          </tr>

          <!-- MOVQ -->
          <tr onmouseout="movq_stop()" onmouseover="movq_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='movq_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/movq_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/movq_teaser.jpg' width="160">
              </div>
              <script type="text/javascript">
                function movq_start() {
                  document.getElementById('movq_image').style.opacity = "1";
                }

                function movq_stop() {
                  document.getElementById('movq_image').style.opacity = "0";
                }
                movq_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/movq">
                <papertitle>MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://scholar.google.com/citations?user=DCC657sAAAAJ&hl=en"> Long Tung Vuong</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a>
              <br>
              <em>NeurIPS (Spotlight)</em>, 2022
              <br>
              <a href="https://chuanxiaz.com/movq">project page</a> /
              <a href="https://openreview.net/pdf?id=Qb-AoSw4Jnm">PDF</a> /
              <a href="https://arxiv.org/abs/2209.09002">arXiv</a> /
              <a href="https://recorder-v3.slideslive.com/?share=75742&s=46fd4642-e778-4156-bc73-151f85156f30">video</a> /
              <a href="https://github.com/ai-forever/Kandinsky-2/tree/main/kandinsky2/vqgan">code(Kandinsky2)</a> /
              <a href="data/2022NeurIPS_poster_MOVQ.png">poster</a>
              <p></p>
              <p>A spatially conditional normalization is introduced to address the repeated artifacts in vector quantized methods.</p>
            </td>
          </tr>

          <!-- ObjectSDF -->
          <tr onmouseout="objectsdf_stop()" onmouseover="objectsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='objectsdf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/objectsdf_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/objectsdf_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function objectsdf_start() {
                  document.getElementById('objectsdf_image').style.opacity = "1";
                }

                function objectsdf_stop() {
                  document.getElementById('objectsdf_image').style.opacity = "0";
                }
                objectsdf_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://wuqianyi.top/objectsdf/">
                <papertitle>Object-Compositional Neural Implicit Surfaces</papertitle>
              </a>
              <br>
              <a href="https://wuqianyi.top/"> Qianyi Wu</a>,
              <a href="https://alvinliu0.github.io/"> Xian Liu</a>,
              <a href="https://donydchen.github.io/"> Yuedong Chen</a>,
              <a href="https://likojack.github.io/kejieli/#/home"> Kejie Li</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://personal.ntu.edu.sg/asjmzheng/">Jianmin Zheng</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://wuqianyi.top/objectsdf/">project page</a> /
              <a href="https://arxiv.org/abs/2207.09686">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=23vxOV19bEw">video</a> /
              <a href="https://github.com/QianyiWu/objsdf">code</a>
              <p></p>
              <p>Automatically decompose a scene into 3D instance, trained using only 2D semantic lables and images.</p>
            </td>
          </tr>

          <!-- Semantic2NeRF -->
          <tr onmouseout="semantic2NeRF_stop()" onmouseover="semantic2NeRF_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='semantic2NeRF_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/semantic2NeRF_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/semantic2NeRF_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function semantic2NeRF_start() {
                  document.getElementById('semantic2NeRF_image').style.opacity = "1";
                }

                function semantic2NeRF_stop() {
                  document.getElementById('semantic2NeRF_image').style.opacity = "0";
                }
                semantic2NeRF_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://donydchen.github.io/sem2nerf/">
                <papertitle>Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/"> Yuedong Chen</a>,
              <a href="https://wuqianyi.top/"> Qianyi Wu</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://donydchen.github.io/sem2nerf/">project page</a> /
              <a href="https://arxiv.org/abs/2203.10821">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=cYr3Dz8N_9E&feature=youtu.be">video</a> /
              <a href="https://github.com/donydchen/sem2nerf">code</a>
              <p></p>
              <p>A 3D inversion model that transfers the 2D semantic map into 3D NeRF, and lets users edit 3D model through 2D semantic input.</p>
            </td>
          </tr>

          <!-- TFill -->
          <tr onmouseout="tfill_stop()" onmouseover="tfill_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tfill_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/tfill_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/tfill_teaser.jpg' width="160">
              </div>
              <script type="text/javascript">
                function tfill_start() {
                  document.getElementById('tfill_image').style.opacity = "1";
                }

                function tfill_stop() {
                  document.getElementById('tfill_image').style.opacity = "0";
                }
                tfill_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
							<a href="https://chuanxiaz.com/tfill">
                <papertitle>Bridging global context interactions for high-fidelity image completion</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://research.monash.edu/en/persons/dinh-phung">Dinh Phung</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
							<a href="https://chuanxiaz.com/tfill">project page</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Bridging_Global_Context_Interactions_for_High-Fidelity_Image_Completion_CVPR_2022_paper.pdf">PDF</a> /
							<a href="https://arxiv.org/abs/2104.00845">arXiv</a> /
							<a href="https://youtu.be/efB1fw0jiLs">video</a> /
							<a href="https://github.com/lyndonzheng/TFill">code</a> /
              <a href="data/2022CVPR_poster_TFill.pdf">poster</a>
              <p></p>
              <p>TFill fills in reasonable contents for both foreground object removal and content completion.</p>
            </td>
          </tr>

          <!-- VinV -->
          <tr onmouseout="vinv_stop()" onmouseover="vinv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vinv_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vinv_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vinv_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function vinv_start() {
                  document.getElementById('vinv_image').style.opacity = "1";
                }

                function vinv_stop() {
                  document.getElementById('vinv_image').style.opacity = "0";
                }
                vinv_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/vinv">
                <papertitle>Visiting the Invisible: Layer-by-Layer Completed Scene Decomposition</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://scholar.google.com/citations?user=AZWFq1sAAAAJ&hl=en">Duy-Son Dao</a>,
              <a href="https://guoxiansong.github.io/homepage/index.html">Guoxian Song</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <br>
              <em>IJCV</em>, 2021
              <br>
              <a href="https://chuanxiaz.com/vinv">project page</a> /
              <a href="https://link.springer.com/article/10.1007/s11263-021-01517-0">PDF</a> /
              <a href="https://arxiv.org/abs/2104.05367">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=QSAYxrKgn7A">video</a> /
              <a href="https://github.com/lyndonzheng/VINV">code</a>
              <p></p>
              <p>A high-level scene understanding system that simultaneously models the completed shape and appearance for all instances.</p>
            </td>
          </tr>

          <!-- AgileGAN -->
          <tr onmouseout="agilegan_stop()" onmouseover="agilegan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='agilegan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/agilegan_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/agilegan_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function agilegan_start() {
                  document.getElementById('agilegan_image').style.opacity = "1";
                }

                function agilegan_stop() {
                  document.getElementById('agilegan_image').style.opacity = "0";
                }
                agilegan_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://guoxiansong.github.io/homepage/agilegan.html">
                <papertitle>AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning</papertitle>
              </a>
              <br>
              <a href="https://guoxiansong.github.io/homepage/index.html">Guoxian Song</a>,
              <a href="http://linjieluo.com/">Linjie Luo</a>,
              <a href="https://www.jingliu.net/">Jing Liu</a>,
              <a href="https://en.wikipedia.org/wiki/Alex_Ma">Wan-Chun Ma</a>,
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/"> Tat-Jen Cham</a>,
              <br>
              <em>SIGGRAPH</em>, 2021
              <br>
              <a href="https://guoxiansong.github.io/homepage/agilegan.html">project page</a> /
              <a href="https://guoxiansong.github.io/homepage/paper/AgileGAN.pdf">PDF</a> /
              <a href="https://www.youtube.com/embed/o0HHIvOT_n0">video</a> /
              <a href="https://github.com/GuoxianSong/AgileGAN">code</a> /
              <a href="http://www.agilegan.com/">Online Demo</a>
              <p></p>
              <p>A GAN inversion model is trained for Stylizing Portraits.</p>
            </td>
          </tr>

          <!-- LSIM -->
          <tr onmouseout="flsesim_stop()" onmouseover="flsesim_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='flsesim_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/flsesim_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/flsesim_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function flsesim_start() {
                  document.getElementById('flsesim_image').style.opacity = "1";
                }
                function flsesim_stop() {
                  document.getElementById('flsesim_image').style.opacity = "0";
                }
                flsesim_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/flsesim">
                <papertitle>The Spatially-Correlative Loss for Various Image Translation Tasks</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://chuanxiaz.com/flsesim">project page</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_The_Spatially-Correlative_Loss_for_Various_Image_Translation_Tasks_CVPR_2021_paper.pdf">PDF</a> /
              <a href="https://arxiv.org/abs/2104.00854">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=pu6PT1om2r0">video</a> /
              <a href="https://github.com/lyndonzheng/F-LSeSim">code</a> /
              <a href="data/2022CVPR_poster_TFill.pdf">poster</a>
              <p></p>
              <p>We propose a novel spatially-correlative loss that is simple, efficient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired I2I translation.</p>
            </td>
          </tr>

          <!-- PICNet -->
          <tr onmouseout="pic_stop()" onmouseover="pic_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pic_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/pic_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/pic_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function pic_start() {
                  document.getElementById('pic_image').style.opacity = "1";
                }
                function pic_stop() {
                  document.getElementById('pic_image').style.opacity = "0";
                }
                pic_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/pic">
                <papertitle>Pluralistic (Free-Form) Image Completion</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>IJCV</em>, 2021
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="https://chuanxiaz.com/pic">project page</a> /
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Pluralistic_Image_Completion_CVPR_2019_paper.pdf">PDF</a> /
              <a href="https://arxiv.org/abs/1903.04227">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs">video</a> /
              <a href="https://github.com/lyndonzheng/Pluralistic-Inpainting">code</a> /
              <a href="data/poster_picnet_cvpr19.pdf">poster</a>
              <p></p>
              <p>Given a single masked image, the proposed model is able to generate multiple and diverse plausible results.</p>
            </td>
          </tr>

          <!-- Syn-to-Real -->
          <tr onmouseout="synthetic2real_stop()" onmouseover="synthetic2real_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='synthetic2real_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/synthetic2real_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/synthetic2real_teaser.png' width="160">
              </div>
              <script type="text/javascript">
                function synthetic2real_start() {
                  document.getElementById('synthetic2real_image').style.opacity = "1";
                }
                function synthetic2real_stop() {
                  document.getElementById('synthetic2real_image').style.opacity = "0";
                }
                synthetic2real_stop()
              </script>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://chuanxiaz.com/synthetic2real">
                <papertitle>T2Net: Synthetic-to-Realistic Translation for Depth Estimation Tasks</papertitle>
              </a>
              <br>
              <strong>Chuanxia Zheng</strong>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="https://chuanxiaz.com/synthetic2real">project page</a> /
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chuanxia_Zheng_T2Net_Synthetic-to-Realistic_Translation_ECCV_2018_paper.pdf">PDF</a> /
              <a href="https://arxiv.org/abs/1808.01454">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=B6lOToIk0xY">video</a> /
              <a href="https://github.com/lyndonzheng/Synthetic2Realistic">code</a> /
              <a href="data/poster_syn2real_eccv18.pdf">poster</a>
              <p></p>
              <p>Without any real depth map, the proposed model evaluates depth maps on real scenes using only synthetic datasets.</p>
            </td>
          </tr>

    </tbody></table>


    <!-- Services -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <heading>Academic Services</heading>
          <p><font size="4"><b>Conference Reviewer</b></font></p>
        <table>
          <tr>
            <td>CVPR</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2020,&nbsp;2021,&nbsp;2022,&nbsp;2023 (Outstanding Reviewer),&nbsp;2024</td>
          </tr>
          <tr><td>ICCV</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2019,&nbsp;2021,&nbsp;2023</td></tr>
          <tr><td>ECCV</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2020,&nbsp;2022,&nbsp;2024</td></tr>
          <tr><td>NeurIPS</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022,&nbsp;2023,&nbsp;2024</td></tr>
          <tr><td>ICLR</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2021,&nbsp;2022,&nbsp;2023,&nbsp;2024</td></tr>
          <tr><td>ICML</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2023</td></tr>
          <tr><td>SIGGRAPH&Asia</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>ICRA</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>IROS</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>IJCAI</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2022</td></tr>
          <tr><td>ACM MM</td><td>&nbsp;&nbsp;&nbsp;&nbsp;2021,&nbsp;2022</td></tr>
        </table>
          <p><font size="4"><b>Journal Reviewer</b></font></p>
          <p>TPAMI, IJCV, TIP, JAS, TMM(Outstanding Reviewer Award, 2021), TCSVT, CVIU, TVCJ, NCAA</p>
        </td>
      </tr>
    </tbody></table>

    <!-- Teaching -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Teaching</heading>
          <ul style="text-align:justify;height: 00px;">
              <li class="font">Teaching Assistant, B16: Software Engineering, Undergraduate, Oxford, 2023</font></li>
              <li class="font">Teaching, Generative AI, Graduate, Oxford Summer School, 2023</font></li>
              <li class="font">Teaching Assistant, Advanced Digital Image Processing, Graduate, NTU, 2018-2020</font></li>
              <li class="font">Teaching Assistant, Human-Computer Interaction, Undergraduate, NTU, 2018-2020</font></li>
              <li class="font">Teaching Assistant, Engineering Mathematics, Undergraduate, NTU, 2018-2020</font></li>
          </ul>
            </td>
          </tr>
        </tbody>
    </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
              <a href="https://jonbarron.info">awesome website template </a><br>
              Last updated May 2024.
          </font>
            </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <script type="text/javascript" src="//ra.revolvermaps.com/0/0/1.js?i=0s5f0y0jjk8&amp;s=180&amp;m=6&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
            </td>
            </tr>
            </tbody></table>

      </td>
    </tr>
  </table>

  </body>
</html>

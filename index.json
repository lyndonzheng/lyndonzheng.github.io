[{"authors":null,"categories":null,"content":"I am a Ph.D. student in the School of Computer Science and Engineering at Nanyang Technological University, advised by Tat-Jen Cham and Jianfei Cai. Before that, I received the Master\u0026rsquo;s degree from Beihang University (Beijing, China) in 2017 and the Bachelor\u0026rsquo;s degree from Beijing Jiaotong University (Beijing, China) in 2014.\nMy research interests are broadly in artificial intelligence, with emphasis on computer vision and machine learning. My current work focuses on scene understanding, especially for image generation, completion and translation, 3D scene understanding, and completion with the goal of building intelligent machines, capable of rebuilding a realistic virtual world.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. student in the School of Computer Science and Engineering at Nanyang Technological University, advised by Tat-Jen Cham and Jianfei Cai. Before that, I received the Master\u0026rsquo;s degree from Beihang University (Beijing, China) in 2017 and the Bachelor\u0026rsquo;s degree from Beijing Jiaotong University (Beijing, China) in 2014.","tags":null,"title":"ChuanXia Zheng","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://www.chuanxiaz.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","[Tat-Jen Cham](http://www.ntu.edu.sg/home/astjcham/)","[Jianfei Cai](http://www.ntu.edu.sg/home/asjfcai/)"],"categories":null,"content":" Video  --  Abstract Bridging distant context interactions is important for high quality image completion with large masks. Previous methods attempting this via deep or large receptive field (RF) convolutions cannot escape from the dominance of nearby interactions, which may be inferior. In this paper, we propose treating image completion as a directionless sequence-to-sequence prediction task, and deploy a transformer to directly capture long-range dependence in the encoder in a first phase. Crucially, we employ a restrictive CNN with small and non-overlapping RF for token representation, which allows the transformer to explicitly model the long-range context relations with equal importance in all layers, without implicitly confounding neighboring tokens when larger RFs are used. In a second phase, to improve appearance consistency between visible and generated regions, a novel attention-aware layer (AAL) is introduced to better exploit distantly related features and also avoid the insular effect of standard attention. Overall, extensive experiments demonstrate superior performance compared to state-of-the-art methods on several datasets.\n Framework  Transformer-based Image Completion Network    The masked image is first resized to a fixed low resolution (256*256) and fed into the transformer to generate semantically correct content. We then merge this inferred content with the original high resolution image and pass it to a refinement network with an Attention-Aware Layer (AAL) to transfer high-quality information from both visible and masked regions.  Token Representation     (a) iGPT downsamples the image to a fixed scale, and embeds each pixel to a token. (b) VIT divides an image to a set of fixed patches and embeds each patch to a token. (c) VQGAN employs a traditional CNN to encode an image to the feature domains and then quantizes each feature as a token through a learned codebook. (d) Ours TFill also encodes an image to the feature domains, yet with a restrictive CNN. To do this, we ensure each token represents only the visible information in a small RF, leaving the long-range dependencies to be explicitly modeled by the transformer encoder in every layer, without cross-contamination from implicit correlation due to larger CNN RF\n    More Results  Results for Center Mask. All images are degraded by center mask. Our model is able to complete both object shape and background scene.     Results for Face Editing. All results are reported at 512*512 resolution. For each pair, the left one is the input image and the right one is the edited output.    Results for Nature Image Editing. Here, we mainly show object removal in traditional image inpainting tasks.   ","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"5faea52101de66a33b16473e058edbda","permalink":"https://www.chuanxiaz.com/publication/tfill/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/tfill/","section":"publication","summary":"Video  --  Abstract Bridging distant context interactions is important for high quality image completion with large masks. Previous methods attempting this via deep or large receptive field (RF) convolutions cannot escape from the dominance of nearby interactions, which may be inferior.","tags":null,"title":"TFill: Image Completion via a Transformer-Based Architecture","type":"publication"},{"authors":["[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","[Tat-Jen Cham](http://www.ntu.edu.sg/home/astjcham/)","[Jianfei Cai](http://www.ntu.edu.sg/home/asjfcai/)"],"categories":null,"content":"  Abstract We propose a novel spatially-correlative loss that is simple, efficient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation. Previous methods attempt this by using pixel-level cycle-consistency or feature-level matching losses, but the domain-specific nature of these losses hinder translation across large domain gaps. To address this, we exploit the spatial patterns of self-similarity as a means of defining scene structure. Our spatially-correlative loss is geared towards only capturing spatial relationships within an image rather than domain appearance. We also introduce a new self-supervised learning method to explicitly learn spatially-correlative maps for each specific translation task. We show distinct improvement over baseline models in all three modes of unpaired I2I translation: single-modal, multi-modal, and even single-image translation. This new loss can easily be integrated into existing network architectures and thus allows wide applicability.\n Method Summary\n Error map visualization  Given an input image, we consider an ideal result (the paired ground truth) and a totally wrong result (another image), repectively. Under such a setting, a good structure loss should penalize the wrong result, while supporting the ideal result. pixel-level loss is naturally unsuitable when there are large domain gaps, and while Perceptual loss will report significant errors for both aligned and unaligned results. PatchNCE mitigates the problem by calculating the cosine distance of features, but it can be seen the loss map still retains high errors in many regions within the aligned result. Our LSeSim has small errors on the left where ground truth paired data is provided, while having large errors on the right for unpaired data.  More Results  Result for single-image translation       Result for visual comparison  ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"eb66a3430cd08bcff560eee628991a2d","permalink":"https://www.chuanxiaz.com/publication/flsesim/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/flsesim/","section":"publication","summary":"Abstract We propose a novel spatially-correlative loss that is simple, efficient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation.","tags":null,"title":"The Spatially-Correlative Loss for Various Image Translation Tasks","type":"publication"},{"authors":["ChuanXia Zheng","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://www.chuanxiaz.com/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","[Tat-Jen Cham](http://www.ntu.edu.sg/home/astjcham/)","[Jianfei Cai](http://www.ntu.edu.sg/home/asjfcai/)"],"categories":null,"content":" Pluralistic Online Demo Currently, demo is not available on mobile devices. Apologize.      if (/Android|webOS|iPhone|iPad|iPod|BlackBerry|BB|PlayBook|IEMobile|Windows Phone|Kindle|Silk|Opera Mini/i.test(navigator.userAgent)) { console.log(\"detect mobile device.\"); document.getElementById('mobile').style.display = \"block\"; document.getElementById('pluralistic').style.display = \"none\"; document.getElementById('range-wrap').style.display = \"none\"; }   Notes:  The Pluralistic model provides multiple and diverse plausible results by clicking the Fill button. For large holes, we can click the Mask Type to select the Rectangle holes. For free-form holes, the left slider can be used to set the line width [1,50]. If we just mask half (left/right) of the face, the diversity will be small for our short+long term attention layer will copy the information from the other slides. Since the data is relayed through the Amazon server to our inner net, the result needs to be waited for a few seconds. Acknowledgments: Online JS code borrows from DeepFill(v1)   Example Editing  Face Change       Eyeglass removal       Old to Young       Object removal      ","date":1552348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552348800,"objectID":"ced241229c332de1b8b6292cda3965b2","permalink":"https://www.chuanxiaz.com/project/pluralistic/","publishdate":"2019-03-12T00:00:00Z","relpermalink":"/project/pluralistic/","section":"project","summary":"Online demo for pluralistic image completion","tags":["Image Completion"],"title":"Pluralistic Image Completion","type":"project"},{"authors":["[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","[Tat-Jen Cham](http://www.ntu.edu.sg/home/astjcham/)","[Jianfei Cai](http://www.ntu.edu.sg/home/asjfcai/)"],"categories":null,"content":"  Video   Abstract Most image completion methods produce only one result for each masked input, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion the task of generating multiple diverse and plausible solutions for image completion. A major challenge faced by learning-based approaches is that here the conditional label itself is a partial image, and there is usually only one ground truth training instance per label. As such, sampling from conditional VAEs still leads to minimal diversity. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that extends the VAE through a latent space that covers all partial images with different mask sizes, and imposes priors that adapt to the number of pixels. The other is a generative path for which the conditional prior is coupled to distributions obtained in the reconstructive path. Both are supported by GANs. We also introduce a new short+long term attention layer that exploits distant relations among decoder and encoder features, improving appearance consistency. When tested on datasets with buildings (Paris), faces (CelebAHQ), and natural images (ImageNet), our method not only generated higher-quality completion results, but also with multiple and diverse plausible outputs.\n Framework  Pluralistic Image Completion Network (PICNet)   Given a masked images, the generative pipeline (blue line) infers the conditional distribution of missing regions, that can be sampled during the testing to generate multiple and diverse results. During the training, the missing regions are encodered to a distribution, that can be sampled to rebuild the original input by combing with the features of visible part (yellow line). This structure is designed on a probabilistically principled freamework. The details can be found in the paper.\n Short+Long Term Attention Layer (SLTAtten)  The proposed Short+Long Term Attention Layer is extended beyond the Self-Attention GAN. This SLTAtten layer not only use the self-attention map within a decoder layer to harness distant spatial context, but further capture feature-feature context between encoder and decoder layers. In this way, the similarity is calculated in the decoder features rather than between the encoder features and decoder features. Our ke novel is: doing so would allow the network a choice of copying information from encoder or the more semantically generative features from the decoder.  More Results  Results for center mask                                                               Results (original, input, output) for object removing                                                             Results (original, input, output) for face playing. When mask left or right face, the diversity will be small for the short+long term attention layer will copy information from other side. When mask top or down face, the diversity will be large.                                  ","date":1552348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552348800,"objectID":"cb670097a277c8a51a685edc94a28184","permalink":"https://www.chuanxiaz.com/publication/pluralistic/","publishdate":"2019-03-12T00:00:00Z","relpermalink":"/publication/pluralistic/","section":"publication","summary":"Video   Abstract Most image completion methods produce only one result for each masked input, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion the task of generating multiple diverse and plausible solutions for image completion.","tags":null,"title":"Pluralistic Image Completion","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://www.chuanxiaz.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","[Tat-Jen Cham](http://www.ntu.edu.sg/home/astjcham/)","[Jianfei Cai](http://www.ntu.edu.sg/home/asjfcai/)"],"categories":null,"content":" Abstract Current methods for single-image depth estimation use training datasets with real image-depth pairs or stereo pairs, which are not easy to acquire. We propose a framework, trained on synthetic imagedepth pairs and unpaired real images, that comprises an image translation network for enhancing realism of input images, followed by a depth prediction network. A key idea is having the first network act as a widespectrum input translator, taking in either synthetic or real images, and ideally producing minimally modified realistic images. This is done via a reconstruction loss when the training input is real, and GAN loss when synthetic, removing the need for heuristic self-regularization. The second network is trained on a task loss for synthetic image-depth pairs, with extra GAN loss to unify real and synthetic feature distributions. Importantly, the framework can be trained end-to-end, leading to good results, even surpassing early deep-learning methods that use real paired data.\n Framework  Video  ","date":1536364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536364800,"objectID":"3768ff8603fdf54e6cecd84577b1c1e0","permalink":"https://www.chuanxiaz.com/publication/synthetic2real/","publishdate":"2018-09-08T00:00:00Z","relpermalink":"/publication/synthetic2real/","section":"publication","summary":"Abstract Current methods for single-image depth estimation use training datasets with real image-depth pairs or stereo pairs, which are not easy to acquire. We propose a framework, trained on synthetic imagedepth pairs and unpaired real images, that comprises an image translation network for enhancing realism of input images, followed by a depth prediction network.","tags":null,"title":"T2Net: Synthetic-to-Realistic Translation for Depth Estimation Tasks","type":"publication"},{"authors":["[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","Jianhua Wang","Weihai Chen","Xingming Wu"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"8aa78bf62667454424dee8451fcfcbb9","permalink":"https://www.chuanxiaz.com/publication/multi-class/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/multi-class/","section":"publication","summary":"Indoor semantic segmentation plays a critical role in many applications, such as intelligent robots. However, multi-class recognition is still challenging, especially for pixel-level indoor semantic labeling. In this paper, a novel deep structured model that combines the strengths of the widely used convolutional neural networks (CNNs) and recurrent neural networks (RNNs) is proposed. We first present a multi-information fusion model that utilizes the scene category information to fine-tune the fully convolutional network. Then, to refine the coarse outputs of CNN, the RNN is applied to the final CNN layer so that we can build an end-to-end trainable system. This Graph-RNN is transformed from a conditional random field based on superpixel segmentation graphical modeling that can utilize flexible contextual information of different neighboring regions. The experimental results on the recent large SUN RGB-D dataset demonstrate that the proposed model outperforms existing state-of-the-art methods on the challenging 40 dominant classes task (40.8% mean IU accuracy and 69.1% pixel accuracy). We also evaluate our model on the public NYU depth V2 dataset and achieve remarkable performance","tags":null,"title":"Multi-class indoor semantic segmentation with deep structured model","type":"publication"},{"authors":["Jianhua Wang","[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","Weihai Chen","Xingming Wu"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"c37d4d174c10c5e53574ff78a1a79f85","permalink":"https://www.chuanxiaz.com/publication/aggregated/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/aggregated/","section":"publication","summary":"Semantic labeling for indoor scenes has been extensively developed with the wide availability of affordable RGB-D sensors. However, it is still a challenging task for multi-class recognition, especially for ‚Äúsmall‚Äù objects. In this paper, a novel semantic labeling model based on aggregated features and contextual information is proposed. Given an RGB-D image, the proposed model first creates a hierarchical segmentation using an adapted gPb/UCM algorithm. Then, a support vector machine is trained to predict initial labels using aggregated features, which fuse small-scale appearance features, mid-scale geometric features, and large-scale scene features. Finally, a joint multi-label Conditional random field model that exploits both spatial and attributive contextual relations is constructed to optimize the initial semantic and attributive predicted results. The experimental results on the public NYU v2 dataset demonstrate the proposed model outperforms the existing state-of-the-art methods on the challenging 40 dominant classes task, and the model also achieves a good performance on a recent SUN RGB-D dataset. Especially, the prediction accuracy of ‚Äúsmall‚Äù classes has been improved significantly","tags":[],"title":"Learning aggregated features and optimizing model for semantic labeling","type":"publication"},{"authors":["[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","Jianhua Wang","Weihai Chen","Xingming Wu"],"categories":null,"content":"","date":1480723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480723200,"objectID":"8db917734274da8f547a2cecae769a8d","permalink":"https://www.chuanxiaz.com/publication/aggregated-c/","publishdate":"2016-12-03T00:00:00Z","relpermalink":"/publication/aggregated-c/","section":"publication","summary":"","tags":null,"title":"Semantic segmentation based on aggregated features and contextual information","type":"publication"},{"authors":["Jianhua Wang","[**Chuanxia Zheng**](https://lyndonzheng.github.io/)","Weihai Chen","Xingming Wu"],"categories":null,"content":"","date":1465084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465084800,"objectID":"482b6bab6de4a90d93d2e5bea1b6646c","permalink":"https://www.chuanxiaz.com/publication/multi-class-c/","publishdate":"2016-06-05T00:00:00Z","relpermalink":"/publication/multi-class-c/","section":"publication","summary":"","tags":null,"title":"Learning contextual information for indoor semantic segmentation\"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://www.chuanxiaz.com/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/","section":"","summary":"","tags":null,"title":"","type":"page"}]